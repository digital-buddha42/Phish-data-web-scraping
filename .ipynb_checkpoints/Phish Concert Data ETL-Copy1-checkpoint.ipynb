{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38cf93cd",
   "metadata": {},
   "source": [
    "# Project 2\n",
    "For project 2, we built an ETL pipeline to create a database containing data on Phish live shows from 1993-2023. First, we scrapped the website phish.net for setlists and the Wikipedia page for Phish concert tours and festivals for attendance and box office data. We then transformed the extracted data, reformatting columns, value formatting, and dropping rows that lacked useful data. We stored all of the results for each year in a variable, which were in turn each stored in a list. To load our data, we converted each variable to a DataFrame and then wrote the DataFrames to CSV files. Finally, we loaded the data from the CSV files directly to SQLite and PostgreSQL. There are many interesting questions that can be explored when analyzing the database we have prepared. For example, we could look at how City and Year affect Attendance and Attendance/Capacity. We could count the recurrence of previous Cities and weigh Attendance and Gross to create a predictive model to determine the likeliest cities to be announced for future show dates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a5b6441",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.types import Integer, Text, String, DateTime, Float\n",
    "import pycountry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5558807",
   "metadata": {},
   "source": [
    "## Part 1: Extract\n",
    "#### Scrapping for Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d736c570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in setlist data\n",
    "url = 'https://phish.net/setlists/phish/'\n",
    "respons = requests.get(url)\n",
    "\n",
    "phish_soup = BeautifulSoup(respons.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3174389",
   "metadata": {},
   "source": [
    "Now we begin scraping the setlist data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99eb71f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['04/21/2024', '04/20/2024', '04/19/2024', '04/18/2024', '02/24/2024']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = phish_soup.find_all('span', class_='setlist-date')\n",
    "\n",
    "date_strings = [date.text[-11:] for date in dates]\n",
    "\n",
    "cleaned_date_strings = [date.strip() for date in date_strings]\n",
    "\n",
    "cleaned_date_strings[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "383dcbd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sphere',\n",
       " 'Sphere',\n",
       " 'Sphere',\n",
       " 'Sphere',\n",
       " 'Moon Palace',\n",
       " 'Moon Palace',\n",
       " 'Moon Palace',\n",
       " 'Moon Palace',\n",
       " 'Moon Palace',\n",
       " 'Madison Square Garden',\n",
       " 'Madison Square Garden',\n",
       " 'Madison Square Garden',\n",
       " 'Madison Square Garden',\n",
       " 'United Center',\n",
       " 'United Center',\n",
       " 'United Center',\n",
       " 'Ervin J. Nutter Center, Wright State University',\n",
       " 'Ervin J. Nutter Center, Wright State University',\n",
       " 'Bridgestone Arena',\n",
       " 'Bridgestone Arena',\n",
       " 'Bridgestone Arena',\n",
       " \"Dick'S Sporting Goods Park\",\n",
       " \"Dick'S Sporting Goods Park\",\n",
       " \"Dick'S Sporting Goods Park\",\n",
       " \"Dick'S Sporting Goods Park\",\n",
       " 'Broadview Stage At Spac',\n",
       " 'Broadview Stage At Spac',\n",
       " 'Madison Square Garden',\n",
       " 'Madison Square Garden',\n",
       " 'Madison Square Garden',\n",
       " 'Madison Square Garden',\n",
       " 'Madison Square Garden',\n",
       " 'Madison Square Garden',\n",
       " 'Madison Square Garden',\n",
       " 'Td Pavilion At The Mann',\n",
       " 'Td Pavilion At The Mann',\n",
       " \"St. Joseph'S Health Amphitheater At Lakeview\",\n",
       " 'The Pavilion At Star Lake',\n",
       " 'The Pavilion At Star Lake',\n",
       " 'Live Oak Bank Pavilion At Riverfront Park',\n",
       " 'Live Oak Bank Pavilion At Riverfront Park',\n",
       " 'Ameris Bank Amphitheatre',\n",
       " 'Ameris Bank Amphitheatre',\n",
       " 'Ameris Bank Amphitheatre',\n",
       " 'Orion Amphitheater',\n",
       " 'Orion Amphitheater',\n",
       " 'Hollywood Bowl',\n",
       " 'Hollywood Bowl',\n",
       " 'Hollywood Bowl',\n",
       " 'William Randolph Hearst Greek Theatre, University Of California, Berkeley',\n",
       " 'William Randolph Hearst Greek Theatre, University Of California, Berkeley',\n",
       " 'William Randolph Hearst Greek Theatre, University Of California, Berkeley',\n",
       " 'Climate Pledge Arena',\n",
       " 'Climate Pledge Arena',\n",
       " 'Moon Palace',\n",
       " 'Moon Palace',\n",
       " 'Moon Palace',\n",
       " 'Moon Palace',\n",
       " 'Madison Square Garden',\n",
       " 'Madison Square Garden',\n",
       " 'Madison Square Garden',\n",
       " 'Madison Square Garden',\n",
       " \"Dick'S Sporting Goods Park\",\n",
       " \"Dick'S Sporting Goods Park\",\n",
       " \"Dick'S Sporting Goods Park\",\n",
       " \"Dick'S Sporting Goods Park\",\n",
       " 'Alpine Valley Music Theatre',\n",
       " 'Alpine Valley Music Theatre',\n",
       " 'Alpine Valley Music Theatre',\n",
       " 'Budweiser Stage',\n",
       " 'Atlantic City Beach',\n",
       " 'Atlantic City Beach',\n",
       " 'Atlantic City Beach',\n",
       " 'Pine Knob Music Theatre',\n",
       " 'Blossom Music Center',\n",
       " 'Merriweather Post Pavilion',\n",
       " 'Merriweather Post Pavilion',\n",
       " 'Coastal Credit Union Music Park At Walnut Creek',\n",
       " 'Northwell Health At Jones Beach Theater',\n",
       " 'Northwell Health At Jones Beach Theater',\n",
       " 'Xfinity Theatre',\n",
       " 'Bethel Woods Center For The Arts',\n",
       " 'Bethel Woods Center For The Arts',\n",
       " 'Td Pavilion At The Mann',\n",
       " 'Td Pavilion At The Mann',\n",
       " 'Maine Savings Amphitheater',\n",
       " 'Xfinity Center',\n",
       " 'Xfinity Center',\n",
       " 'Ruoff Music Center',\n",
       " 'Ruoff Music Center',\n",
       " 'Ruoff Music Center']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venues = phish_soup.find_all('div', class_='setlist-venue')\n",
    "\n",
    "try:\n",
    "    venue_names = [venue.text.strip().title() for venue in venues]\n",
    "except Exception as e:\n",
    "    venue_names = [venue.find('span').text.strip() for venue in venues]\n",
    "\n",
    "venue_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9b144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafba087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping for known exceptions\n",
    "exception_mapping = {\n",
    "    \"Quintana Roo\": \"MX\",\n",
    "    \"Cancun\": \"MX\",  # Add other exceptions as needed\n",
    "}\n",
    "\n",
    "def get_country_code(location):\n",
    "    # Check if the location is in the exception mapping\n",
    "    if location in exception_mapping:\n",
    "        return exception_mapping[location]\n",
    "    try:\n",
    "        # Check if the location is a valid country name\n",
    "        return pycountry.countries.lookup(location).alpha_2\n",
    "    except LookupError:\n",
    "        # If the country is not found, return None\n",
    "        return None\n",
    "\n",
    "def get_state_or_country(location):\n",
    "    # Split the location by comma to separate city and state/country\n",
    "    parts = location.split(',')\n",
    "    city = parts[0].strip().title()\n",
    "    if len(parts) > 1:\n",
    "        state_or_country = parts[1].strip().title()\n",
    "        # Try to get country code\n",
    "        country_code = get_country_code(state_or_country)\n",
    "        if country_code:\n",
    "            return city, country_code\n",
    "        else:\n",
    "            # If not a country, assume it's a state\n",
    "            return city, state_or_country.upper()\n",
    "    return city, None\n",
    "\n",
    "locations = phish_soup.find_all('div', class_='setlist-location')\n",
    "locations = [location.text.strip() for location in locations]\n",
    "\n",
    "cities_states = [get_state_or_country(location) for location in locations]\n",
    "cities = [city for city, _ in cities_states]\n",
    "states_or_countries = [state_or_country for _, state_or_country in cities_states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(states_or_countries[:5])\n",
    "print(cities_states[:5])\n",
    "print(cities[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7e19ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# phish_p1_df = pd.DataFrame({\n",
    "#     'Date': cleaned_date_strings,\n",
    "#     'Venue': venue_names,\n",
    "#     'City': cities,\n",
    "#     'State': states\n",
    "# })\n",
    "# phish_p1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a24f307",
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = 'https://phish.net/setlists/?year='\n",
    "all_years_dates = []\n",
    "all_venues = []\n",
    "all_cities = []\n",
    "all_states = []\n",
    "\n",
    "# Get the current year\n",
    "current_year = datetime.now().year\n",
    "\n",
    "# Loop from 1982 to the current year\n",
    "for year in range(1982, current_year + 1):\n",
    "    \n",
    "    year_url = url2 + str(year)\n",
    "    \n",
    "    respons = requests.get(year_url)\n",
    "\n",
    "    phish_soup = BeautifulSoup(respons.text, 'html.parser')\n",
    "    \n",
    "    # Dates\n",
    "    dates = phish_soup.find_all('span', class_='setlist-date')\n",
    "\n",
    "    date_strings = [date.text[-11:] for date in dates]\n",
    "\n",
    "#     cleaned_date_strings = [date.strip().strftime('%Y%m%d') for date in date_strings]\n",
    "    cleaned_date_strings = [datetime.strptime(date.strip(), '%m/%d/%Y').strftime('%Y-%m-%d') for date in date_strings]\n",
    "\n",
    "    \n",
    "    all_years_dates.extend(cleaned_date_strings)\n",
    "    \n",
    "    # Venues\n",
    "    venues = phish_soup.find_all('div', class_='setlist-venue')\n",
    "\n",
    "    venue_strings = [venue.find('span').text.strip().title().replace(\"'S\", \"'s\") for venue in venues]\n",
    "    \n",
    "    all_venues.extend(venue_strings)\n",
    "    \n",
    "    # Locations/ City/ State\n",
    "    locations = phish_soup.find_all('div', class_='setlist-location')\n",
    "\n",
    "    locations = [location.text.strip() for location in locations]\n",
    "\n",
    "    cities = [location.split(',')[0].title() for location in locations]\n",
    "    all_cities.extend(cities)\n",
    "    \n",
    "    states = [location[-2:].upper() for location in locations]\n",
    "    all_states.extend(states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37b14449",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'state_or_country' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m cities_states \u001b[38;5;241m=\u001b[39m [get_state_or_country(location) \u001b[38;5;28;01mfor\u001b[39;00m location \u001b[38;5;129;01min\u001b[39;00m locations]\n\u001b[0;32m     64\u001b[0m cities \u001b[38;5;241m=\u001b[39m [city \u001b[38;5;28;01mfor\u001b[39;00m city, _ \u001b[38;5;129;01min\u001b[39;00m cities_states]\n\u001b[1;32m---> 65\u001b[0m states_or_countries \u001b[38;5;241m=\u001b[39m [state_or_country \u001b[38;5;28;01mfor\u001b[39;00m _, state_or_countries \u001b[38;5;129;01min\u001b[39;00m cities_states]\n\u001b[0;32m     67\u001b[0m all_cities\u001b[38;5;241m.\u001b[39mextend(cities)\n\u001b[0;32m     68\u001b[0m all_states\u001b[38;5;241m.\u001b[39mextend(states_or_countries)\n",
      "Cell \u001b[1;32mIn[5], line 65\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     63\u001b[0m cities_states \u001b[38;5;241m=\u001b[39m [get_state_or_country(location) \u001b[38;5;28;01mfor\u001b[39;00m location \u001b[38;5;129;01min\u001b[39;00m locations]\n\u001b[0;32m     64\u001b[0m cities \u001b[38;5;241m=\u001b[39m [city \u001b[38;5;28;01mfor\u001b[39;00m city, _ \u001b[38;5;129;01min\u001b[39;00m cities_states]\n\u001b[1;32m---> 65\u001b[0m states_or_countries \u001b[38;5;241m=\u001b[39m [state_or_country \u001b[38;5;28;01mfor\u001b[39;00m _, state_or_countries \u001b[38;5;129;01min\u001b[39;00m cities_states]\n\u001b[0;32m     67\u001b[0m all_cities\u001b[38;5;241m.\u001b[39mextend(cities)\n\u001b[0;32m     68\u001b[0m all_states\u001b[38;5;241m.\u001b[39mextend(states_or_countries)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'state_or_country' is not defined"
     ]
    }
   ],
   "source": [
    "url2 = 'https://phish.net/setlists/?year='\n",
    "all_years_dates = []\n",
    "all_venues = []\n",
    "all_cities = []\n",
    "all_states = []\n",
    "\n",
    "# Define a mapping for known exceptions\n",
    "exception_mapping = {\n",
    "    \"Quintana Roo\": \"MX\",\n",
    "    \"Cancun\": \"MX\",  # Add other exceptions as needed\n",
    "}\n",
    "\n",
    "def get_country_code(location):\n",
    "    # Check if the location is in the exception mapping\n",
    "    if location in exception_mapping:\n",
    "        return exception_mapping[location]\n",
    "    try:\n",
    "        # Check if the location is a valid country name\n",
    "        return pycountry.countries.lookup(location).alpha_2\n",
    "    except LookupError:\n",
    "        # If the country is not found, return None\n",
    "        return None\n",
    "\n",
    "def get_state_or_country(location):\n",
    "    # Split the location by comma to separate city and state/country\n",
    "    parts = location.split(',')\n",
    "    city = parts[0].strip().title()\n",
    "    if len(parts) > 1:\n",
    "        state_or_country = parts[1].strip().title()\n",
    "        # Try to get country code\n",
    "        country_code = get_country_code(state_or_country)\n",
    "        if country_code:\n",
    "            return city, country_code\n",
    "        else:\n",
    "            # If not a country, assume it's a state\n",
    "            return city, state_or_country.upper()\n",
    "    return city, None\n",
    "\n",
    "# Get the current year\n",
    "current_year = datetime.now().year\n",
    "\n",
    "# Loop from 1982 to the current year\n",
    "for year in range(1982, current_year + 1):\n",
    "    year_url = url2 + str(year)\n",
    "    response = requests.get(year_url)\n",
    "    phish_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Dates\n",
    "    dates = phish_soup.find_all('span', class_='setlist-date')\n",
    "    date_strings = [date.text[-11:] for date in dates]\n",
    "    cleaned_date_strings = [datetime.strptime(date.strip(), '%m/%d/%Y').strftime('%Y-%m-%d') for date in date_strings]\n",
    "    all_years_dates.extend(cleaned_date_strings)\n",
    "\n",
    "    # Venues\n",
    "    venues = phish_soup.find_all('div', class_='setlist-venue')\n",
    "    venue_strings = [venue.find('span').text.strip().title().replace(\"'S\", \"'s\") for venue in venues]\n",
    "    all_venues.extend(venue_strings)\n",
    "\n",
    "    # Locations/ City/ State\n",
    "    locations = phish_soup.find_all('div', class_='setlist-location')\n",
    "    locations = [location.text.strip() for location in locations]\n",
    "\n",
    "    cities_states = [get_state_or_country(location) for location in locations]\n",
    "    cities = [city for city, _ in cities_states]\n",
    "    states_or_countries = [state_or_country for _, state_or_countries in cities_states]\n",
    "\n",
    "    all_cities.extend(cities)\n",
    "    all_states.extend(states_or_countries)\n",
    "\n",
    "print(all_years_dates[:5])\n",
    "print(all_venues[:5])\n",
    "print(all_cities[:5])\n",
    "print(all_states[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a0ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "phish_shows_df = pd.DataFrame({\n",
    "    'Date': all_years_dates,\n",
    "    'Venue': all_venues,\n",
    "    'City': all_cities,\n",
    "    'State': all_states\n",
    "})\n",
    "phish_shows_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25da3b35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
